# -*- coding: utf-8 -*-
"""Submission: Image Classification Model Deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jSjLJyzyURWXshtcxvbqKFkXuaeomgj

- Nama Lengkap : Adisaputra Zidha Noorizki
- Username : hi_zidha
- Email : hi.zidha@gmail.com
"""

!pip install tensorflow
!pip install keras

# Dataset: https://www.kaggle.com/datasets/marquis03/bdd100k-weather-classification

"""Import Dataset"""

!pip install split-folders

import os, shutil, zipfile, splitfolders
from google.colab import drive
drive.mount('/content/drive')

zip_path = "/content/drive/MyDrive/Colab Notebooks/IDCamp2023/Intermediate/dataset/dataset.zip"
extracted_path = "/content/drive/MyDrive/Colab Notebooks/IDCamp2023/Intermediate/dataset/datasetIntermediate"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_path)

extracted_path += "/dataset"
ready_dataset_path = "/content/drive/MyDrive/Colab Notebooks/IDCamp2023/Intermediate/dataset/datasetIntermediate/readyDataset"
os.makedirs(ready_dataset_path, exist_ok=True)

subfolders = ['overcast', 'partly cloudy', 'rainy', 'snowy']
target_file_count = 2600

for subfolder in subfolders:
    source_folder = os.path.join(extracted_path, subfolder)
    target_folder = os.path.join(ready_dataset_path, subfolder)

    os.makedirs(target_folder, exist_ok=True)
    files = os.listdir(source_folder)
    selected_files = files[:target_file_count]

    for file_name in selected_files:
        source_path = os.path.join(source_folder, file_name)
        target_path = os.path.join(target_folder, file_name)
        shutil.move(source_path, target_path)

print("Proses selesai.")

input_folder = ready_dataset_path
output_folder = '/content/drive/MyDrive/Colab Notebooks/IDCamp2023/Intermediate/dataset/datasetIntermediate/finalDataset'

splitfolders.ratio(input_folder,
                   output_folder,
                   seed=42,
                   ratio=(0.8, 0.2))

train_dir = output_folder+'/train'
val_dir   = output_folder+'/val'

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import keras
import numpy as np

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

from keras.models import Sequential
from keras.optimizers import Adam
from keras.losses import categorical_crossentropy
from keras.metrics import Accuracy
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard

"""Pre-processing Data"""

datagen = ImageDataGenerator(rescale=1./255)

train_gen = datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=128,
    class_mode='categorical',
    subset='training'
)

validation_gen = datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=128,
    class_mode='categorical',
    shuffle=False
)

"""Build a Model"""

batch_size = 128
input_shape = (224, 224, 3)

early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('weatherClassification.h5', save_best_only=True, mode='max')
tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)

model = Sequential([
    Input(shape=input_shape),

    Conv2D(64, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),
    Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),

    Conv2D(256, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),
    Conv2D(256, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),

    Conv2D(512, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),
    Conv2D(512, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),
    Conv2D(512, kernel_size=3, strides=1, activation='relu', padding='same'),
    MaxPooling2D(2, 2),

    Flatten(),

    Dropout(0.25),
    BatchNormalization(),

    Dense(256, activation='relu'),
    Dense(4, activation='softmax')
], name='modelCNN')

model.summary()

"""Setting and Compile Model"""

optimizers = Adam(learning_rate=0.001)
model.compile(optimizer=optimizers, loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_gen,
    steps_per_epoch=48,
    validation_data=validation_gen,
    validation_steps=16,
    epochs=50,
    callbacks=[early_stopping, model_checkpoint, tensorboard_callback]
)

model.save("weatherClassification.h5", include_optimizer=True)

"""Plotting the model accuracy and loss"""

plt.figure(figsize=(15, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')

plt.tight_layout()
plt.show()

"""Convert Model format from '.h5' to TF-Lite"""

models = tf.keras.models.load_model('weatherClassification.h5')

converter = tf.lite.TFLiteConverter.from_keras_model(models)
tflite_model = converter.convert()

with open('weatherModels.tflite', 'wb') as f:
    f.write(tflite_model)